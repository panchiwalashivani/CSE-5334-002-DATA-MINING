{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment # 4 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When submitting, fill your full name, your student ID and your NetID in this cell. Note that this is a markdown cell! \n",
    "\n",
    "Student Full Name: Shivani Manojkumar Panchiwala\n",
    "\n",
    "ID: 1001982478\n",
    "\n",
    "Team Mate name : Savaliya Kuldip Rameshbhai\n",
    "\n",
    "ID: 1001832000\n",
    "\n",
    "2) Meghaben Ghanshyambhai Patel\n",
    "\n",
    "ID: 1002006777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Work is to be done in a team\n",
    "2. Any cheating including plagiarism, cooperation will be reported to the corresponding UTA’ s instance.\n",
    "3. If using any resource (books, internet), please make sure that you cite it.\n",
    "4. Follow the given structure. Specifically, place all your tasks in THIS NOTEBOOK BUT IN SEPARATE BLOCKS. Then save this notebook as 'yourNetID_pa3.ipynb' and submit it. \n",
    "5. Do not alter the dataset name.\n",
    "6. Please dont ask any details specific to the project like \"How to plot XYZ ? What parameters are to be used? \" and so on..\n",
    "7. Report is required for this assignment. Still you need to comment your code. (-10 for no comments in the code.)\n",
    "8. Please dont send images of your visualizations to verify whether they are right or not before submission deadline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The purpose of this assignment is to cluster  using K-means clustering and Hierarchical Agglomerative clustering models and to visualize clusters for predicted and actual cluster labels.\n",
    "\n",
    "\n",
    "Your dataset is given as 2 files(clustering.csv and college.csv).<br>\n",
    "Output variable for clustering.csv - Class (<=50K and >50K)<br>\n",
    "Output variable for college.csv - Grad.Rate (<=50 and >50k)<br>\n",
    "\n",
    "You need to submit this ipython file after renaming it. \n",
    "\n",
    "Preprocessing will be needed for the data as most of the data is in string and needs to be quantified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required Python packages here\n",
    "# Seaborn,numpy,pandas,sklearn,matplotlib only\n",
    "-20 points for the use of libraries other than the above mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaborn,numpy,pandas,sklearn,matplotlib only\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"College.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,~df.columns.str.match(\"Unnamed\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Private'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_En = LabelEncoder()\n",
    "df['Private'] = label_En.fit_transform(df['Private'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1-a: Determine “k” value from the elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will be using the elbow method to determine the optimal number of clusters for k-means clustering.\n",
    "\n",
    "We need some way to determine whether we are using the right number of clusters when using k-means clustering. One method to validate the number of clusters is the elbow method. \n",
    "\n",
    "The idea of the elbow method is to run k-means clustering on the dataset for a range of values of k (k will be from 1 to 10 in this task), and for each value of k calculate the sum of squared errors (SSE). Then, plot a line chart of the SSE for each value of k. If the line chart looks like an arm, then the \"elbow\" on the arm is the value of k that is the best. The idea is that we want a small SSE, but that the SSE tends to decrease toward 0 as we increase k (the SSE is 0 when k is equal to the number of data points in the dataset, because then each data point is a cluster, and there is no error between it and the center of its cluster). So our goal is to choose a small value of k that still has a low SSE, and the elbow usually represents where we start to have diminishing returns by increasing k.\n",
    "\n",
    "For this task, you need to perform the elbow method for k from 1 to 10 and plot a line chart of the SSE for each value of k, and determine the best k (the number of clusters). Note that you need to use the whole dataset in this task and you need to print your decision for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################begin code for Task 1-a\n",
    "Sum_of_squared_errors = []\n",
    "K = range(1,11)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(df)\n",
    "    Sum_of_squared_errors.append(km.inertia_)\n",
    "\n",
    "plt.plot(K, Sum_of_squared_errors)\n",
    "plt.plot(K, Sum_of_squared_errors, 'x', color='Red')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_errors')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "#########################begin code for Task 1-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Elbow method we are varying the number of clusters K from 1-10 and for each value of K sum of squared errors between each point and centroid in a cluster. Here in the above plot we can see that as the number of clusters increases the SSE starts decreasing.\n",
    "\n",
    "Here we took the Optimal value of K as 3 because before 3 variation changes rapidly but after 3 it slows down leading to an elbow formation in the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1-b: Visualization for K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will be performing k-means clustering for k=2 and visualize the predicted training samples and actual training samples on scatter plots. Use 70% of the dataset for training and 30% of the dataset for testing. Perform kmeans for clustering samples in your training set. \n",
    "\n",
    "Use two subplots for visualizing the predicted training samples and actual training samples on two scatter plots.\n",
    "\n",
    "Since your dataset has multiple features(dimensions), you won't be able to plot your data on a scatter plot. Thus, you’re going to visualize your data with the help of one of the Dimensionality Reduction techniques, namely Principal Component Analysis (PCA). The idea in PCA is to find a linear combination of the two variables that contains most of the information. This new variable or “principal component” can replace the two original variables. You can easily apply PCA to your data with the help of scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################begin code for Task 1-b-1: Split the dataset 70% for training and 30% for testing\n",
    "# random state = 2022\n",
    "### Important!!!\n",
    "from sklearn.model_selection import train_test_split\n",
    "### Important!!!\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=2,init='k-means++',max_iter=300, n_init=12, random_state=2022)\n",
    "kmeans_final.fit(X_pca)\n",
    "df[\"clusters\"] = kmeans_final.labels_\n",
    "df_last = df[\"clusters\"]\n",
    "X = df[df.columns[:-1]]\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, df_last, test_size = 0.3, random_state = 2022)\n",
    "###################end code for Task 1-b-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdScaler = StandardScaler()\n",
    "X_train = stdScaler.fit_transform(X_train)\n",
    "X_test = stdScaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################begin code for Task 1-b-2: Visualize the predicted training labels vs actual training labels\n",
    "\n",
    "# Import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Create the KMeans model\n",
    "km_model_k2 = KMeans(n_clusters=2, random_state=2022, algorithm='elkan')\n",
    "y = km_model_k2.fit(X_train_pca)\n",
    "\n",
    "# Compute cluster centers and predict cluster index for each sample \n",
    "c_centers = y.cluster_centers_\n",
    "c_centers\n",
    "\n",
    "c_index = y.predict(X_train_pca)\n",
    "c_index\n",
    "\n",
    "c_labels = y.labels_\n",
    "\n",
    "# Model and fit the data to the PCA model\n",
    "# X_train_pca = None\n",
    "\n",
    "# Visualize the predicted training labels vs actual training labels. \n",
    "### scatter(x, y, your_data)\n",
    "x = X_train_pca[:, 0]\n",
    "y = X_train_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "c_unique = np.unique(c_index)\n",
    "for i in c_unique:\n",
    "  plt.scatter(x[c_index==i], y[c_index==i], label=i, s=15)\n",
    "\n",
    "plt.scatter(c_centers[:,0], c_centers[:,1], marker=\"X\", color=\"c\")\n",
    "plt.title('Visualizing the Predicted training samples with best K=2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x, y, c=c_labels, s=15)\n",
    "\n",
    "plt.title('Visualizing the Actual training samples')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "###################end code for Task 1-b-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to visualize the predicted testing labels versus actual testing labels. Use the trained model in previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################begin code for Task 1-b-3: Visualize the predicted testing labels vs actual testing labels\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# predict cluster index for each sample \n",
    "\n",
    "# Model and fit the data to the PCA model\n",
    "#X_test_pca = None\n",
    "X_test_pca = pca.fit_transform(X_test)\n",
    "y = km_model_k2.fit(X_test_pca)\n",
    "\n",
    "\n",
    "c_centers = y.cluster_centers_\n",
    "c_centers\n",
    "\n",
    "c_index = y.predict(X_test_pca)\n",
    "c_index\n",
    "\n",
    "c_labels = y.labels_\n",
    "\n",
    "# Visualize the predicted testing labels vs actual testing labels. \n",
    "### scatter(x, y, your_data)\n",
    "x = X_test_pca[:, 0]\n",
    "y = X_test_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "c_unique_t = np.unique(c_index)\n",
    "for i in c_unique_t:\n",
    "  plt.scatter(x[c_index==i], y[c_index==i], label=i, s=15)\n",
    "\n",
    "plt.scatter(c_centers[:,0], c_centers[:,1], marker=\"X\", color=\"c\")\n",
    "plt.title('Visualizing the Predicted Test labels with best K=2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x, y, c=c_labels, s=15)\n",
    "plt.title('Visualizing the Actual Test labels')\n",
    "plt.show()\n",
    "\n",
    "###################end code for Task 1-b-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is the scatter plot of predicted labels versus actual labels for testing data where I have run K-means on the whole dataset and assigned the actual labels dividing the data in three clusters for k=2. Moreover, I have then predicted the labels. Black Cross are the centroids of the clusters respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you need to provide the evaluation of your clustering model. Print out a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################begin code for Task 1-b-4: Print out a confusion matrix\n",
    "confus_matx = confusion_matrix(c_labels,c_index) \n",
    "print(\"Confusion Matrix for Testing data with k=2\") \n",
    "print(confus_matx) \n",
    "\n",
    "###################end code for Task 1-b-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: Hierarchical Agglomerative  Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2-a: Find the best Hierarchical Agglomerative Clustering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will be performing Hierarchical Agglomerative clustering with different linkage methods (complete and average) and different similarity measures (cosine, euclidean, and manhattan) in order to find the best pair of linkage method and similarity measure. Use F1 score for evaluation and take n_clusters = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################begin code for Task 2-a: Print out a confusion matrix\n",
    "# Import AgglomerativeClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Import pairwise_distances for calculating pairwise distance matrix\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# Import f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "Kmeans_task2 = KMeans(n_clusters=2, random_state=2022)\n",
    "kmeans_t2 = Kmeans_task2.fit_predict(X_pca)\n",
    "df[\"clusters\"] = kmeans_t2\n",
    "X_2a = df[df.columns[:-1]]\n",
    "Y_2a = df['clusters'].copy()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_2a, Y_2a, test_size = 0.3, random_state = 2022)\n",
    "\n",
    "stdScaler = StandardScaler()\n",
    "X_train = stdScaler.fit_transform(X_train)\n",
    "X_test = stdScaler.fit_transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "\n",
    "\n",
    "## Calculate pairwise distance matrix for X_train\n",
    "pdm_train = X_train_pca\n",
    "cos_dis = pairwise_distances(pdm_train, metric='cosine')\n",
    "euclid_dis = pairwise_distances(pdm_train, metric='euclidean')\n",
    "manhat_dis = pairwise_distances(pdm_train, metric='manhattan')\n",
    "\n",
    "## Model and fit the training data to the AgglomerativeClustering model\n",
    "## complete linkage + cosine\n",
    "cos_complete = AgglomerativeClustering(n_clusters=2, linkage='complete', affinity='precomputed').fit_predict(cos_dis)\n",
    "f1_cos_complete = f1_score(Y_train, cos_complete, average='weighted')\n",
    "\n",
    "## Model and fit the training data to the AgglomerativeClustering model\n",
    "## complete linkage + euclidean\n",
    "euclid_complete = AgglomerativeClustering(n_clusters=2, linkage='complete', affinity='precomputed').fit_predict(euclid_dis)\n",
    "f1_euclid_complete = f1_score(Y_train, euclid_complete, average='weighted')\n",
    "\n",
    "## Model and fit the training data to the AgglomerativeClustering model\n",
    "## complete linkage + manhattan\n",
    "manhat_complete = AgglomerativeClustering(n_clusters=2, linkage='complete', affinity='precomputed').fit_predict(manhat_dis)\n",
    "f1_manhat_complete = f1_score(Y_train, manhat_complete, average='weighted')\n",
    "\n",
    "## Model and fit the training data to the AgglomerativeClustering model\n",
    "## average linkage + cosine\n",
    "cos_avg = AgglomerativeClustering(n_clusters=2, linkage='average', affinity='precomputed').fit_predict(cos_dis)\n",
    "f1_cos_avg = f1_score(Y_train, cos_avg, average='weighted')\n",
    "\n",
    "## Model and fit the training data to the AgglomerativeClustering model\n",
    "## average linkage + euclidean\n",
    "euclid_avg = AgglomerativeClustering(n_clusters=2, linkage='average', affinity='precomputed').fit_predict(euclid_dis)\n",
    "f1_euclid_avg = f1_score(Y_train, euclid_avg, average='weighted')\n",
    "\n",
    "## Model and fit the training data to the AgglomerativeClustering model\n",
    "## average linkage + manhattan\n",
    "manhat_avg = AgglomerativeClustering(n_clusters=2, linkage='average', affinity='precomputed').fit_predict(manhat_dis)\n",
    "f1_manhat_avg = f1_score(Y_train, manhat_avg, average='weighted')\n",
    "\n",
    "## Print the 2-D confusion matrix with different linkage methods (complete and average) and different similarity measures (cosine, euclidean, and manhattan) in order to find the best pair of linkage method and similarity measure. \n",
    "\n",
    "print(\"F1-score for complete linkage + cosine\\t\\t\", f1_cos_complete)\n",
    "print(\"F1-score for complete linkage + euclidean\\t\", f1_euclid_complete)\n",
    "print(\"F1-score for complete linkage + manhattan\\t\", f1_manhat_complete)\n",
    "print(\"F1-score for average linkage + cosine\\t\\t\", f1_cos_avg)\n",
    "print(\"F1-score for average linkage + euclidean\\t\", f1_euclid_avg)\n",
    "print(\"F1-score for average linkage + manhattan\\t\", f1_manhat_avg)\n",
    "\n",
    "###################end code for Task 2-a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2-b:  Visualization for Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best performed model from the previous step and use that model for visualizing the predicted training samples and actual training samples on scatter plots. Use PCA model for visualizing your data (use X_train_pca from Task 1-b-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################begin code for Task 2-b: Visualize the predicted training labels vs actual training labels\n",
    "models = {\n",
    "    'cosine_complete':f1_cos_complete,\n",
    "    'euc_complete':f1_euclid_complete,\n",
    "    'manhattan_complete':f1_manhat_complete,\n",
    "    'cos_avg':f1_cos_avg,\n",
    "    'euc_avg':f1_euclid_avg,\n",
    "    'man_avg':f1_manhat_avg  }\n",
    "best_model_val = max(models,key=models.get)\n",
    "\n",
    "if best_model_val == 'cosine_complete':\n",
    "    best_model = cos_complete\n",
    "\n",
    "elif best_model_val == 'euc_complete':\n",
    "    best_model = euclid_complete\n",
    "\n",
    "elif best_model_val == 'manhattan_complete':\n",
    "    best_model = manhat_complete\n",
    "\n",
    "elif best_model_val == 'cos_avg':\n",
    "    best_model = cos_avg\n",
    "\n",
    "elif best_model_val == 'euc_avg':\n",
    "    best_model = euclid_avg\n",
    "\n",
    "elif best_model_val == 'man_avg':\n",
    "    best_model = manhat_avg\n",
    "\n",
    "else:\n",
    "    best_model = None\n",
    "\n",
    "\n",
    "# Visualize the predicted training labels versus actual training labels.\n",
    "x = X_train_pca[:,0]\n",
    "y = X_train_pca[:,1]\n",
    "\n",
    "print(\"Visualize using the best model:\\t\", best_model_val)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x,y,c=best_model,s=15)\n",
    "print(\"\\n\")\n",
    "plt.title(\"Visualize using the best model\")\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x, y, c=Y_train, s=15)\n",
    "plt.title('Visualizing the Actual Test labels')\n",
    "plt.show()\n",
    "###################end code for Task 2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3:  Compare K-Means Clustering and Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3-a: Visualize Clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, use whole dataset for training k-means cluster and hierarchical agglomerative clustering. Use the best model for agglomerative clustering. Visualize the predicted labels from k-means clustering and agglomerative clustering versus actual labels. Basically, you need to plot three scatter plots as subplots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################begin code for Task 3-a: Visualize the predicted training labels vs actual training labels\n",
    "\n",
    "### Kmeans Clustering\n",
    "# Model and fit the data to the Kmeans (use fit_predict : Performs clustering on X and returns cluster labels.)\n",
    "Kmeans_task3 = KMeans(n_clusters=2, random_state=2022)\n",
    "kmeans_t3 = Kmeans_task3.fit_predict(X_pca)\n",
    "\n",
    "\n",
    "### Agglomerative Clustering\n",
    "# Calculate pairwise distance matrix for X\n",
    "cos_dis1 = pairwise_distances(X_pca, metric='cosine')\n",
    "\n",
    "cos_comp1 = AgglomerativeClustering(n_clusters=2,linkage='complete',affinity='precomputed').fit_predict(cos_dis1)\n",
    "\n",
    "x = X_pca[:, 0]\n",
    "y = X_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x,y,c=kmeans_t3,s=15)\n",
    "plt.title(\"Visualize using KMeans (n_cluster=2):\")\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"Visualize using the Agglomerative: cosine_complete\")\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x,y,c=cos_comp1,s=15)\n",
    "plt.title(\"Visualize using the best model for agglomerative clustering\")\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Visualize using Actual Labels:\")\n",
    "plt.scatter(x,y,c=kmeans_t2,s=15)\n",
    "plt.show()\n",
    "print(\"\\n\")\n",
    "\n",
    "###################end code for Task 3-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is the scatter plot of predicted labels by kmean and Agglomerative clustering versus actual labels for traning data. The actual labels are divided in two clusters. Moreover, I have then predicted the labels. Black Cross are the centroids of the clusters respectively. The model divides the data into two clusters for kmeans no. of clusters 2 & Cosine Complete method which is displayed in the first two subplot of graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3-b: Compare K-Means Clustering &  Hierarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out confusion matrices for kmeans and agglomerative clustering. Also, compare precision, recall, and F1-score for both model. Type your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################begin code for Task 3-b\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "print(\"Confusion matrix:\\n\")\n",
    "conf_kmeans = confusion_matrix(kmeans_t2,kmeans_t3)\n",
    "conf_agglo = confusion_matrix(kmeans_t2,cos_comp1)\n",
    "\n",
    "#Print confusion matrix of kmeans\n",
    "print(\"K-means:\\n\")\n",
    "print(conf_kmeans)\n",
    "\n",
    "#Print confusion matrix of Agglomerative\n",
    "print(\"\\nAgglomerative:\\n\")\n",
    "print(conf_agglo)\n",
    "\n",
    "print(f\"\\nWe can conclude that: \\n \\t - Both models identify {conf_agglo[0][0]} samples of cluster 1 and {conf_agglo[1][1]} samples of cluster 2 as belonging to the same cluster\")\n",
    "print(f\"\\t - Whereas there is dissimilarity in the prediction of {conf_agglo[0][1]+conf_agglo[1][0]} samples where, \\n\\t\\t {conf_agglo[0][1]} samples are predicted differently in cluster 1 and {conf_agglo[1][0]} samples are predicted differently in cluster 2\")\n",
    "print(f\"\\nHere we can also conclude that: \\n \\t - In the Kmeans model it is identifying {conf_kmeans[0][0]} True Positives and {conf_kmeans[0][1]} False Positives which makes the precison 1 and \\n\\t\\tin the case of Agglomerative model it indentifies {conf_agglo[0][0]} True Positives and {conf_agglo[0][1]} False Positives which makes the precision 0.57\")\n",
    "print(f\"\\t - In the Kmeans model it is identifying {conf_kmeans[0][0]} True Positives and {conf_kmeans[1][0]} False Negatives which makes the Recall 1 and \\n\\t\\tin the case of Agglomerative model it indentifies {conf_agglo[0][0]} True Positives and {conf_agglo[1][0]} False Negatives which makes the Recall 0.79\")\n",
    "\n",
    "#Print Precision of Kmeans and Agglomerative\n",
    "print(\"\\nPrecision:\\n\")\n",
    "prec_kmeans = precision_score(kmeans_t2,kmeans_t3, average='micro')\n",
    "prec_agglo = precision_score(kmeans_t2,cos_comp1, average='micro')\n",
    "print(\"Kmeans:\", prec_kmeans, \"\\tAgglomerative:\", prec_agglo)\n",
    "\n",
    "\n",
    "if prec_kmeans > prec_agglo:\n",
    "    print(\"K-means Clustering method has higher precision value than Agglomerative Clustering\\n\")\n",
    "else:\n",
    "    print(\"Agglomerative Clustering method has higher precision value than K-means Clustering\\n\")\n",
    "\n",
    "#Print F1 Score of Kmeans and Agglomerative\n",
    "print(\"\\nF1 Score\\n\")\n",
    "F1_kmeans = f1_score(kmeans_t2,kmeans_t3,average='weighted')\n",
    "F1_agglo = f1_score(kmeans_t2,cos_comp1,average='weighted')\n",
    "print(\"Kmeans:\", F1_kmeans, \"\\tAgglomerative:\", F1_agglo)\n",
    "\n",
    "\n",
    "#Print Recall of Kmeans and Agglomerative\n",
    "print(\"\\nRecall\\n\")\n",
    "recall_kmeans = recall_score(kmeans_t2, kmeans_t3, average='weighted')\n",
    "recall_agglo = recall_score(kmeans_t2, cos_comp1, average='weighted')\n",
    "print(\"Kmeans:\", recall_kmeans, \"\\tAgglomerative:\", recall_agglo)\n",
    "\n",
    "if recall_kmeans > recall_agglo:\n",
    "  print(\"K-means Clustering method has higher recall value than Agglomerative Clustering\\n\")\n",
    "else:\n",
    "    print(\"Agglomerative Clustering method has higher recall value than K-means Clustering\\n\")\n",
    "\n",
    "print(\"\")\n",
    "###################end code for Task 3-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>[05 points] Follow the Rules</b> \n",
    "\n",
    "\n",
    "<b>[35 points] Task 1:</b>  \n",
    "\n",
    "    [10 points] Task 1-a: Determine “k” value from the elbow method\n",
    "\n",
    "    [25 points] Task 1-b: Visualization for K-Means Clustering\n",
    "\n",
    "        [02 points] Task 1-b-1: Split the dataset \n",
    "    \n",
    "        [10 points] Task 1-b-2: Visualize the predicted training vs actual training labels \n",
    "    \n",
    "        [10 points] Task 1-b-3: Visualize the predicted testing vs actual testing labels\n",
    "    \n",
    "        [03 points] Task 1-b-4: Print out a confusion matrix\n",
    "    \n",
    "\n",
    "\n",
    "<b>[45 points] Task 2:</b>  \n",
    "\n",
    "    [35 points] Task 2-a: Find the best Hierarchical Agglomerative Clustering Model\n",
    "\n",
    "    [10 points] Task 2-b: Visualization for Hierarchical Agglomerative Clustering\n",
    "\n",
    "\n",
    "\n",
    "<b>[15 points] Report :</b> \n",
    "Make a 4 min video explaining both the assignment and it's implementation.\n",
    "\n",
    "\n",
    " <b>[20 points] Task 3 (BONUS):</b> \n",
    "\n",
    "    Task 3-a: 10 points\n",
    "\n",
    "    Task 3-b: 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
